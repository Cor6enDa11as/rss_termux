#!/usr/bin/env python3
import os
import json
import feedparser
import requests
import time
import logging
import random
from datetime import datetime, timezone, timedelta

# ==================== –ó–∞–≥—Ä—É–∑–∫–∞ –Ω–∞—Å—Ç—Ä–æ–µ–∫ ====================
BOT_TOKEN = os.getenv('BOT_TOKEN')
CHANNEL_ID = os.getenv('CHANNEL_ID')

if not BOT_TOKEN or not CHANNEL_ID:
    print("‚ùå –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ BOT_TOKEN –∏ CHANNEL_ID –≤ GitHub Secrets!")
    exit(1)

CONFIG = {
    'REQUEST_DELAY_MIN': 10,   # ‚úÖ –£–í–ï–õ–ò–ß–ò–õ–ò —Å 5
    'REQUEST_DELAY_MAX': 15,  # ‚úÖ –£–í–ï–õ–ò–ß–ò–õ–ò —Å 10
    'MAX_HOURS_BACK': 24
}

# ==================== –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ ====================
FEEDS = {}

# ==================== –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è ====================
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ==================== –§—É–Ω–∫—Ü–∏–∏ ====================

def load_rss_feeds():
    """üì∞ –ó–∞–≥—Ä—É–∂–∞–µ—Ç RSS-–ª–µ–Ω—Ç—ã –∏ —Ö—ç—à—Ç–µ–≥–∏"""
    global FEEDS
    feeds = {}

    try:
        with open('feeds.txt', 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if not line or line.startswith('#'):
                    continue

                if '#' in line:
                    url, tag = line.split('#', 1)
                    feeds[url.strip()] = '#' + tag.strip()
                else:
                    feeds[line] = '#–Ω–æ–≤–æ—Å—Ç–∏'

    except FileNotFoundError:
        logger.error("‚ùå –§–∞–π–ª feeds.txt –Ω–µ –Ω–∞–π–¥–µ–Ω")
        exit(1)

    if not feeds:
        logger.error("‚ùå –ù–µ—Ç RSS-–ª–µ–Ω—Ç")
        exit(1)

    FEEDS = feeds
    logger.info(f"üì∞ –ó–∞–≥—Ä—É–∂–µ–Ω–æ: {len(feeds)} –ª–µ–Ω—Ç")

def load_dates():
    """üìÅ –ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏—Å—Ç–æ—Ä–∏—é –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã—Ö –ø–æ—Å—Ç–æ–≤"""
    try:
        with open('dates.json', 'r', encoding='utf-8') as f:
            data = json.load(f)
            for url, info in data.items():
                if 'last_date' in info:
                    info['last_date'] = datetime.fromisoformat(info['last_date'])
            return data
    except FileNotFoundError:
        return {}

def save_dates(dates_dict):
    """üíæ –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∏—Å—Ç–æ—Ä–∏—é –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã—Ö –ø–æ—Å—Ç–æ–≤"""
    data_to_save = {}
    for url, info in dates_dict.items():
        if isinstance(info, dict) and 'last_date' in info:
            data_to_save[url] = {'last_date': info['last_date'].isoformat()}

    with open('dates.json', 'w', encoding='utf-8') as f:
        json.dump(data_to_save, f, indent=2, ensure_ascii=False)

def send_to_telegram(title, link, feed_url, entry):
    """üì® –û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏–µ (–ü–†–ï–í–¨–Æ –°–í–ï–†–•–£!)"""
    try:
        clean_title = (title.replace('&', '&amp;')
                          .replace('<', '&lt;')
                          .replace('>', '&gt;')
                          .replace('"', '&quot;')
                          .replace("'", '&#39;'))
        hashtag = FEEDS.get(feed_url, '#–Ω–æ–≤–æ—Å—Ç–∏')

        author = getattr(entry, 'author', '')
        if author:
            author_hashtag = author.replace(" ", "")
            message = f'<a href="{link}">{clean_title}</a>\n\nüìå {hashtag} üë§ #{author_hashtag}'
        else:
            message = f'<a href="{link}">{clean_title}</a>\n\nüìå {hashtag}'

        data = {
            'chat_id': CHANNEL_ID,
            'text': message,
            'parse_mode': 'HTML',
            'link_preview_options': json.dumps({
                'is_disabled': False,
                'url': link,
                'show_above_text': True
            })
        }

        response = requests.post(f'https://api.telegram.org/bot{BOT_TOKEN}/sendMessage',
                               data=data, timeout=10)

        if response.status_code == 200:
            return True
        else:
            logger.error(f"‚ùå TG –æ—Ç–≤–µ—Ç: {response.status_code}")
            return False

    except Exception as e:
        logger.error(f"ü§ñ –û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏: {e}")
        return False

def parse_feed(url):
    """üì∞ –ü–∞—Ä—Å–∏—Ç RSS-–ª–µ–Ω—Ç—É"""
    try:
        headers = {'User-Agent': 'Mozilla/5.0', 'Accept': 'application/rss+xml'}
        response = requests.get(url, headers=headers, timeout=10)
        if response.status_code != 200:
            logger.error(f"‚ùå HTTP {response.status_code}: {url[:40]}...")
            return None
        feed = feedparser.parse(response.content)
        return feed if hasattr(feed, 'entries') and feed.entries else None
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ {url[:40]}...: {e}")
        return None

def get_entry_date(entry):
    """üìÖ –ü–æ–ª—É—á–∞–µ—Ç –¥–∞—Ç—É –∑–∞–ø–∏—Å–∏"""
    if hasattr(entry, 'published_parsed') and entry.published_parsed:
        return datetime(*entry.published_parsed[:6], tzinfo=timezone.utc)
    return datetime.now(timezone.utc)

# ==================== –û—Å–Ω–æ–≤–Ω–∞—è –ª–æ–≥–∏–∫–∞ ====================

def check_feeds():
    """üîç –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –ª–µ–Ω—Ç"""
    logger.info("=" * 60)
    logger.info(f"ü§ñ [{len(FEEDS)} –ª–µ–Ω—Ç] {datetime.now().strftime('%H:%M')}")
    start_time = time.time()

    dates = load_dates()
    sent_count = 0

    for feed_url in FEEDS:
        try:
            logger.info(f"üì∞ –ü—Ä–æ–≤–µ—Ä–∫–∞: {feed_url[:50]}...")

            last_date = dates.get(feed_url, {}).get('last_date')
            threshold_date = last_date if last_date else \
                (datetime.now(timezone.utc) - timedelta(hours=CONFIG['MAX_HOURS_BACK']))

            feed = parse_feed(feed_url)
            if not feed:
                time.sleep(random.uniform(CONFIG['REQUEST_DELAY_MIN'], CONFIG['REQUEST_DELAY_MAX']))
                continue

            new_entries = []
            for entry in feed.entries:
                entry_date = get_entry_date(entry)
                if entry_date > threshold_date:
                    new_entries.append((entry, entry_date))

            if new_entries:
                logger.info(f"  üì¶ –ù–∞–π–¥–µ–Ω–æ –Ω–æ–≤—ã—Ö: {len(new_entries)}")
                new_entries.sort(key=lambda x: x[1])

                max_date = threshold_date
                for entry, pub_date in new_entries:
                    title = getattr(entry, 'title', '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è')
                    link = getattr(entry, 'link', '')

                    if not link:
                        continue

                    logger.info(f"  üì§ –û—Ç–ø—Ä–∞–≤–∫–∞ [{pub_date.strftime('%H:%M')}]: {title[:60]}...")

                    if send_to_telegram(title, link, feed_url, entry):
                        sent_count += 1
                        if pub_date > max_date:
                            max_date = pub_date

                        # ‚úÖ –ù–û–í–û–ï: –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –ø–æ—Å—Ç–∞–º–∏ (2-4 —Å–µ–∫)
                        time.sleep(random.uniform(5, 10))
                    else:
                        logger.error("  ‚ùå –û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏")
                        # ‚úÖ –ü—Ä–∏ 429 - –ø–∞—É–∑–∞ 10 —Å–µ–∫
                        time.sleep(10)

                if max_date > threshold_date:
                    dates[feed_url] = {'last_date': max_date}
                    save_dates(dates)
            else:
                logger.info(f"  ‚úÖ –ù–µ—Ç –Ω–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π")

            time.sleep(random.uniform(CONFIG['REQUEST_DELAY_MIN'], CONFIG['REQUEST_DELAY_MAX']))

        except Exception as e:
            logger.error(f"  ‚ùå –û—à–∏–±–∫–∞: {e}")
            time.sleep(random.uniform(CONFIG['REQUEST_DELAY_MIN'], CONFIG['REQUEST_DELAY_MAX']))
            continue

    logger.info(f"üìä –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –û—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ: {sent_count} –Ω–æ–≤–æ—Å—Ç–µ–π")
    logger.info(f"‚è± –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {time.time() - start_time:.1f} —Å–µ–∫")
    logger.info("=" * 60)
    return sent_count

# ==================== –ó–∞–ø—É—Å–∫ ====================

if __name__ == '__main__':
    logger.info("=" * 60)
    load_rss_feeds()
    logger.info(f"‚è∞ –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏: {CONFIG['REQUEST_DELAY_MIN']}-{CONFIG['REQUEST_DELAY_MAX']} —Å–µ–∫")
    logger.info(f"‚è≥ –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–æ–≤–æ—Å—Ç–∏ –∑–∞: {CONFIG['MAX_HOURS_BACK']} —á–∞—Å–æ–≤")
    logger.info("=" * 60)

    sent_count = check_feeds()
    logger.info(f"‚úÖ –ë–æ—Ç –∑–∞–≤–µ—Ä—à–∏–ª —Ä–∞–±–æ—Ç—É. –û—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ: {sent_count} –ø–æ—Å—Ç–æ–≤")
