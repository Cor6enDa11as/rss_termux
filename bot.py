#!/usr/bin/env python3

import os
import json
import feedparser
import requests
import time
import logging
import random
from datetime import datetime, timezone, timedelta
from dotenv import load_dotenv
from urllib.parse import urlparse

# ==================== –ó–ê–ì–†–£–ó–ö–ê .env ====================
load_dotenv()

# ==================== –ù–ê–°–¢–†–û–ô–ö–ò ====================
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('bot.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# –ü–æ–ª—É—á–∞–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∏–∑ .env
BOT_TOKEN = os.getenv('BOT_TOKEN')
CHANNEL_ID = os.getenv('CHANNEL_ID')

# ==================== –ó–ê–ì–†–£–ó–ö–ê RSS –õ–ï–ù–¢ ====================
def load_rss_feeds():
    """üì∞ –ó–∞–≥—Ä—É–∂–∞–µ—Ç RSS –ª–µ–Ω—Ç—ã –∏–∑ .env —Ñ–∞–π–ª–∞ - –ü–†–û–°–¢–û –ò –ë–´–°–¢–†–û"""
    feeds_str = os.getenv('RSS_FEEDS', '')

    if not feeds_str:
        logger.error("‚ùå RSS_FEEDS –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ .env —Ñ–∞–π–ª–µ!")
        exit(1)

    # –†–∞–∑–±–∏–≤–∞–µ–º –ø–æ –ø–µ—Ä–µ–Ω–æ—Å—É —Å—Ç—Ä–æ–∫–∏
    feeds = []
    for line in feeds_str.split('\n'):
        line = line.strip()
        if line and not line.startswith('#'):
            feeds.append(line)

    if not feeds:
        logger.error("‚ùå –ù–µ—Ç RSS –ª–µ–Ω—Ç –≤ .env —Ñ–∞–π–ª–µ!")
        exit(1)

    logger.info(f"üì∞ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(feeds)} RSS –ª–µ–Ω—Ç")
    return feeds

RSS_FEEDS = load_rss_feeds()

# –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
if not BOT_TOKEN or not CHANNEL_ID:
    logger.error("‚ùå –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ BOT_TOKEN –∏ CHANNEL_ID –≤ .env —Ñ–∞–π–ª–µ!")
    exit(1)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∏–∑ .env
REQUEST_DELAY_MIN = int(os.getenv('REQUEST_DELAY_MIN', '2'))
REQUEST_DELAY_MAX = int(os.getenv('REQUEST_DELAY_MAX', '5'))
MAX_HOURS_BACK = int(os.getenv('MAX_HOURS_BACK', '48'))  # –ó–∞ —Å–∫–æ–ª—å–∫–æ —á–∞—Å–æ–≤ –ø—Ä–æ–≤–µ—Ä—è–µ–º –Ω–æ–≤–æ—Å—Ç–∏

# –°–ª—É—á–∞–π–Ω—ã–µ User-Agent
USER_AGENTS = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',
    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36',
]

# ==================== –£–ü–†–û–©–ï–ù–ù–´–ï –§–£–ù–ö–¶–ò–ò ====================
def load_dates():
    """üìÅ –ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏—Å—Ç–æ—Ä–∏—é –∏–∑ dates.json - –ü–†–û–°–¢–û"""
    try:
        with open('dates.json', 'r', encoding='utf-8') as f:
            data = json.load(f)
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º —Å—Ç—Ä–æ–∫–∏ –¥–∞—Ç –≤ datetime
            for url, info in data.items():
                if isinstance(info, dict) and 'last_date' in info:
                    info['last_date'] = datetime.fromisoformat(info['last_date'])
            return data
    except FileNotFoundError:
        logger.info("üìÅ dates.json –Ω–µ –Ω–∞–π–¥–µ–Ω, –Ω–∞—á–∏–Ω–∞–µ–º —Å —á–∏—Å—Ç–æ–≥–æ –ª–∏—Å—Ç–∞")
        return {}
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ dates.json: {e}")
        return {}

def save_dates(dates_dict):
    """üíæ –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∏—Å—Ç–æ—Ä–∏—é –≤ dates.json - –ü–†–û–°–¢–û"""
    try:
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º datetime –≤ —Å—Ç—Ä–æ–∫–∏ –¥–ª—è JSON
        data_to_save = {}
        for url, info in dates_dict.items():
            if isinstance(info, dict) and 'last_date' in info and isinstance(info['last_date'], datetime):
                data_to_save[url] = {
                    'last_date': info['last_date'].isoformat()
                }
            else:
                data_to_save[url] = info

        with open('dates.json', 'w', encoding='utf-8') as f:
            json.dump(data_to_save, f, indent=2, ensure_ascii=False)
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è dates.json: {e}")

def is_russian_text_simple(text):
    """üî§ –ü–†–û–°–¢–ê–Ø –ø—Ä–æ–≤–µ—Ä–∫–∞ —Ä—É—Å—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞"""
    if not text:
        return False
    # –ü—Ä–æ—Å—Ç–æ –ø—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ä—É—Å—Å–∫–∏—Ö –±—É–∫–≤
    russian_letters = set('–∞–±–≤–≥–¥–µ—ë–∂–∑–∏–π–∫–ª–º–Ω–æ–ø—Ä—Å—Ç—É—Ñ—Ö—Ü—á—à—â—ä—ã—å—ç—é—è')
    text_lower = text.lower()
    russian_count = sum(1 for char in text_lower if char in russian_letters)
    total_letters = sum(1 for char in text_lower if char.isalpha())

    if total_letters == 0:
        return False

    return russian_count > 0  # –•–æ—Ç—è –±—ã –æ–¥–Ω–∞ —Ä—É—Å—Å–∫–∞—è –±—É–∫–≤–∞

def translate_text_simple(text):
    """üåê –ü–†–û–°–¢–û–ô –ø–µ—Ä–µ–≤–æ–¥ —á–µ—Ä–µ–∑ Google Translate"""
    try:
        if not text or len(text) < 3:
            return text, False

        url = "https://translate.googleapis.com/translate_a/single"
        params = {
            'client': 'gtx',
            'sl': 'auto',
            'tl': 'ru',
            'dt': 't',
            'q': text[:500]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É
        }

        response = requests.get(url, params=params, timeout=5)
        if response.status_code == 200:
            translated = response.json()[0][0][0]
            if translated and translated.strip() and translated != text:
                return translated, True

        return text, False
    except Exception:
        return text, False

def create_simple_hashtag(url):
    """üè∑Ô∏è –ü–†–û–°–¢–û–ô —Ö—ç—à—Ç–µ–≥ –∏–∑ URL - —Ç–æ–ª—å–∫–æ —É–±–∏—Ä–∞–µ–º _ –∏ -"""
    try:
        parsed = urlparse(url)
        domain = parsed.netloc.lower()

        # –£–¥–∞–ª—è–µ–º www.
        if domain.startswith('www.'):
            domain = domain[4:]

        # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—É—é —á–∞—Å—Ç—å –¥–æ–º–µ–Ω–∞ –¥–æ —Ç–æ—á–∫–∏
        source_name = domain.split('.')[0]

        # –£–±–∏—Ä–∞–µ–º _ –∏ - (–ø—Ä–æ—Å—Ç–æ –∑–∞–º–µ–Ω—è–µ–º –Ω–∞ –ø—É—Å—Ç–æ—Ç—É)
        hashtag_name = source_name.replace('_', '').replace('-', '')

        # –î–æ–±–∞–≤–ª—è–µ–º —ç–º–æ–¥–∑–∏ —â–∏—Ç–∞ üõ°Ô∏è –ø–µ—Ä–µ–¥ —Ö—ç—à—Ç–µ–≥–æ–º
        return f"üè∑Ô∏è #{hashtag_name}"
    except Exception:
        return "üõ°Ô∏è #news"

def send_to_telegram_simple(title, link, source_url):
    """üì® –ü–†–û–°–¢–ê–Ø –æ—Ç–ø—Ä–∞–≤–∫–∞ –≤ Telegram"""
    try:
        # –≠–∫—Ä–∞–Ω–∏—Ä—É–µ–º HTML
        clean_title = (title
                      .replace('&', '&amp;')
                      .replace('<', '&lt;')
                      .replace('>', '&gt;')
                      .replace('"', '&quot;'))

        # –°–æ–∑–¥–∞–µ–º —Ö—ç—à—Ç–µ–≥
        hashtag = create_simple_hashtag(source_url)

        # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ: üöÄ –ø–µ—Ä–µ–¥ –∑–∞–≥–æ–ª–æ–≤–∫–æ–º, üõ°Ô∏è –ø–µ—Ä–µ–¥ —Ö—ç—à—Ç–µ–≥–æ–º
        message = f'üöÄ <a href="{link}">{clean_title}</a> {hashtag}'

        response = requests.post(
            f'https://api.telegram.org/bot{BOT_TOKEN}/sendMessage',
            data={
                'chat_id': CHANNEL_ID,
                'text': message,
                'parse_mode': 'HTML',
                'disable_web_page_preview': False
            },
            timeout=10
        )

        return response.status_code == 200

    except Exception as e:
        logger.error(f"ü§ñ –û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏: {e}")
        return False

def parse_feed_simple(url):
    """üì∞ –ü–†–û–°–¢–û–ô –ø–∞—Ä—Å–∏–Ω–≥ RSS - –±–µ–∑ —Å–ª–æ–∂–Ω—ã—Ö –ø—Ä–æ–≤–µ—Ä–æ–∫ –∫–æ–¥–∏—Ä–æ–≤–∫–∏"""
    try:
        headers = {
            'User-Agent': random.choice(USER_AGENTS),
            'Accept': 'application/rss+xml, application/atom+xml, application/xml, text/xml',
        }

        # –ü–†–û–°–¢–û: feedparser —Å–∞–º —Ä–∞–∑–±–µ—Ä–µ—Ç—Å—è —Å –∫–æ–¥–∏—Ä–æ–≤–∫–æ–π
        feed = feedparser.parse(url, request_headers=headers, timeout=10)

        # –ë–∞–∑–æ–≤–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞
        if hasattr(feed, 'bozo') and feed.bozo:
            logger.debug(f"‚ö†Ô∏è –ü—Ä–æ–±–ª–µ–º—ã —Å –ø–∞—Ä—Å–∏–Ω–≥–æ–º {url[:40]}...")
            return None

        return feed

    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ {url[:40]}...: {e}")
        return None

def get_entry_date_simple(entry):
    """üìÖ –ü–†–û–°–¢–û–ï –ø–æ–ª—É—á–µ–Ω–∏–µ –¥–∞—Ç—ã –∑–∞–ø–∏—Å–∏"""
    try:
        # –ü—Ä–æ–±—É–µ–º published_parsed
        if hasattr(entry, 'published_parsed') and entry.published_parsed:
            return datetime(*entry.published_parsed[:6], tzinfo=timezone.utc)

        # –ü—Ä–æ–±—É–µ–º updated_parsed
        if hasattr(entry, 'updated_parsed') and entry.updated_parsed:
            return datetime(*entry.updated_parsed[:6], tzinfo=timezone.utc)

        # –ï—Å–ª–∏ –Ω–µ—Ç –Ω–æ—Ä–º–∞–ª—å–Ω–æ–π –¥–∞—Ç—ã - –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ–∫—É—â—É—é
        return datetime.now(timezone.utc)

    except Exception:
        return datetime.now(timezone.utc)

# ==================== –û–°–ù–û–í–ù–ê–Ø –õ–û–ì–ò–ö–ê ====================
def check_feeds_simple():
    """üîç –ü–†–û–°–¢–ê–Ø –ø—Ä–æ–≤–µ—Ä–∫–∞ –≤—Å–µ—Ö RSS –ª–µ–Ω—Ç"""
    logger.info("=" * 60)
    logger.info("üöÄ –ù–∞—á–∞–ª–æ –ø—Ä–æ–≤–µ—Ä–∫–∏ –Ω–æ–≤–æ—Å—Ç–µ–π")
    start_time = time.time()

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é
    dates = load_dates()
    sent_count = 0

    # –î–ª—è –∫–∞–∂–¥–æ–π –ª–µ–Ω—Ç—ã
    for feed_url in RSS_FEEDS:
        try:
            logger.info(f"üì∞ –ü—Ä–æ–≤–µ—Ä–∫–∞: {feed_url[:50]}...")

            # –ü–æ–ª—É—á–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é –∏–∑–≤–µ—Å—Ç–Ω—É—é –¥–∞—Ç—É
            last_date = None
            if feed_url in dates:
                last_date = dates[feed_url].get('last_date')

            # –ü–∞—Ä—Å–∏–º –ª–µ–Ω—Ç—É (–ü–†–û–°–¢–û!)
            feed = parse_feed_simple(feed_url)
            if not feed or not hasattr(feed, 'entries') or not feed.entries:
                logger.error(f"  ‚ùå –ü—É—Å—Ç–∞—è –∏–ª–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞—è –ª–µ–Ω—Ç–∞")
                continue

            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–æ—Ä–æ–≥–æ–≤—É—é –¥–∞—Ç—É
            # –ï—Å–ª–∏ —ç—Ç–æ –ø–µ—Ä–≤–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ - —Å–º–æ—Ç—Ä–∏–º —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏–µ MAX_HOURS_BACK —á–∞—Å–æ–≤
            if last_date is None:
                threshold_date = datetime.now(timezone.utc) - timedelta(hours=MAX_HOURS_BACK)
            else:
                threshold_date = last_date

            # –ò—â–µ–º –Ω–æ–≤—ã–µ –∑–∞–ø–∏—Å–∏
            new_entries_found = 0

            for entry in feed.entries:
                # –ü–æ–ª—É—á–∞–µ–º –¥–∞—Ç—É –∑–∞–ø–∏—Å–∏ (–ü–†–û–°–¢–û!)
                entry_date = get_entry_date_simple(entry)

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–æ–≤–∞—è –ª–∏ –∑–∞–ø–∏—Å—å
                if entry_date > threshold_date:
                    # –ü–æ–ª—É—á–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
                    title = getattr(entry, 'title', '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è')
                    link = getattr(entry, 'link', '')

                    if not link:
                        continue

                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏ –ø–µ—Ä–µ–≤–æ–¥–∏–º –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
                    if not is_russian_text_simple(title):
                        translated, success = translate_text_simple(title)
                        if success:
                            title = translated
                            logger.debug(f"  üåê –ü–µ—Ä–µ–≤–µ–¥–µ–Ω–æ: {title[:50]}...")

                    # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º (–ü–†–û–°–¢–û!)
                    logger.info(f"  üì§ –û—Ç–ø—Ä–∞–≤–∫–∞: {title[:60]}...")
                    if send_to_telegram_simple(title, link, feed_url):
                        sent_count += 1
                        new_entries_found += 1

                        # –û–±–Ω–æ–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é –¥–∞—Ç—É –¥–ª—è —ç—Ç–æ–π –ª–µ–Ω—Ç—ã
                        dates[feed_url] = {
                            'last_date': entry_date
                        }

                        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ —É—Å–ø–µ—à–Ω–æ–≥–æ –ø–æ—Å—Ç–∞
                        save_dates(dates)

                        # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –ø–æ—Å—Ç–∞–º–∏
                        time.sleep(random.uniform(REQUEST_DELAY_MIN, REQUEST_DELAY_MAX))
                    else:
                        logger.error(f"  ‚ùå –û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏")
                        break  # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–ø—Ä–∞–≤–∏—Ç—å - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –æ—Å—Ç–∞–ª—å–Ω—ã–µ –ø–æ—Å—Ç—ã —ç—Ç–æ–π –ª–µ–Ω—Ç—ã

            if new_entries_found:
                logger.info(f"  ‚úÖ –ù–∞–π–¥–µ–Ω–æ –Ω–æ–≤—ã—Ö: {new_entries_found}")
            else:
                if last_date:
                    logger.info(f"  ‚è≥ –ù–µ—Ç –Ω–æ–≤—ã—Ö (–ø–æ—Å–ª–µ–¥–Ω—è—è: {last_date.strftime('%d.%m %H:%M')})")
                else:
                    logger.info(f"  ‚úÖ –õ–µ–Ω—Ç–∞ –ø—Ä–æ–∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")

            # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –ª–µ–Ω—Ç–∞–º–∏
            time.sleep(random.uniform(REQUEST_DELAY_MIN, REQUEST_DELAY_MAX))

        except Exception as e:
            logger.error(f"  ‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ª–µ–Ω—Ç—ã: {e}")
            time.sleep(random.uniform(REQUEST_DELAY_MIN, REQUEST_DELAY_MAX))
            continue

    # –§–∏–Ω–∞–ª—å–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
    save_dates(dates)

    logger.info(f"üìä –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
    logger.info(f"‚úÖ –û—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ –ø–æ—Å—Ç–æ–≤: {sent_count}")
    logger.info(f"‚è± –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {time.time() - start_time:.1f} —Å–µ–∫")
    logger.info("=" * 60)

    return sent_count

# ==================== –ó–ê–ü–£–°–ö ====================
if __name__ == '__main__':
    logger.info("=" * 60)
    logger.info("üöÄ –£–ü–†–û–©–ï–ù–ù–´–ô RSS Bot –∑–∞–ø—É—â–µ–Ω")
    logger.info(f"üì∞ –û—Ç—Å–ª–µ–∂–∏–≤–∞–µ—Ç—Å—è –ª–µ–Ω—Ç: {len(RSS_FEEDS)}")
    logger.info(f"‚è∞ –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏: {REQUEST_DELAY_MIN}-{REQUEST_DELAY_MAX} —Å–µ–∫")
    logger.info(f"‚è≥ –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–æ–≤–æ—Å—Ç–∏ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ: {MAX_HOURS_BACK} —á–∞—Å–æ–≤")
    logger.info("=" * 60)

    # –ó–∞–ø—É—Å–∫–∞–µ–º –ø—Ä–æ–≤–µ—Ä–∫—É
    sent_count = check_feeds_simple()

    logger.info(f"‚úÖ –ë–æ—Ç –∑–∞–≤–µ—Ä—à–∏–ª —Ä–∞–±–æ—Ç—É")
    logger.info(f"üì® –í—Å–µ–≥–æ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ: {sent_count} –ø–æ—Å—Ç–æ–≤")
    logger.info("üí° –ù–∞—Å—Ç—Ä–æ–π—Ç–µ cron –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –∑–∞–ø—É—Å–∫–∞:")
    logger.info("   */20 * * * * cd /path/to/bot && python3 bot.py")
